{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jiyun-kong/outta_AI_bootcamp/blob/main/nlp_main_final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4I5LBN5Zfl7Y"
      },
      "outputs": [],
      "source": [
        "!pip install import_ipynb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ivndl4z_J7zI"
      },
      "source": [
        "***주의사항***  \n",
        "  \n",
        "이하의 Google Drive를 mount하는 코드는, nlp_main_final.ipynb 파일을 colab 환경에서 실행할 때 필요한 코드이다.  \n",
        "그러나 본 파일을 colab에서 최종 확인한 후, Jupyter notebook에서 main_stream_final.ipynb 파일을 실행할 때에는 아래 코드가 작동되어서는 안 된다. Jupyter notebook에서 main_stream_final.ipynb 파일을 실행할 때, 본 파일의 predict_sentence 함수가 import되어 사용되는데, 아래 코드가 Jupyter 환경에서 오류를 발생시킬 수 있기 때문이다.  \n",
        "따라서, **본 파일을 colab에서 성공적으로 완성시킨 후에는, 아래의 Drive mount 코드를 주석 처리하거나 삭제한 후에 main_stream_final.ipynb 파일을 실행하여야 한다.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fzg41GykJ-24"
      },
      "outputs": [],
      "source": [
        "# 본 파일을 colab 환경에서 실행할 때만 필요한 코드\n",
        "# 본 파일을 완성한 후에는 주석처리/삭제 함으로써, 이후 Jupyter notebook 환경에서 main_stream_final.ipynb 파일을 실행시킬 때 오류가 발생하지 않도록 할 것\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "%cd /content/drive/MyDrive/NLP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nTqcsIWyfqLQ"
      },
      "outputs": [],
      "source": [
        "import import_ipynb, torch\n",
        "\n",
        "from nlp_Preproc_final import data_to_token_ids, token_ids_to_mask\n",
        "from nlp_model_final import get_model\n",
        "from nlp_tokenization import KoBertTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7AQdbcbOsHQk"
      },
      "outputs": [],
      "source": [
        "# GPU 디바이스 설정\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = get_model(device, torch.cuda.is_available())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "25DF1S9UFelR"
      },
      "outputs": [],
      "source": [
        "def predict_sentence(input_str):\n",
        "    \n",
        "    # 입력받은 문장을 토큰화\n",
        "    tokenizer = KoBertTokenizer.from_pretrained(\"monologg/kobert\")\n",
        "    tokenized_data = [data_to_token_ids(tokenizer, input_str)]\n",
        "    attention_masks = [token_ids_to_mask(token_ids) for token_ids in tokenized_data]\n",
        "\n",
        "    # 토큰을 텐서로 변환\n",
        "    inputs_tensor = torch.tensor(tokenized_data).to(device)\n",
        "    masks_tensor = torch.tensor(attention_masks).to(device)\n",
        "\n",
        "    # 신경망의 Forward 함수 활용\n",
        "    # 위의 두 텐서와 'model'의 forward 함수를 이용해서 감정에 대한 확률 벡터를 산출해보자.\n",
        "    # 이때 산출된 확률 벡터를 outputs 변수로 저장하자.\n",
        "    outputs = model(inputs_tensor, attention_mask=masks_tensor)\n",
        "\n",
        "    # 위에서 산출한 감정 확률 벡터에 softmax 함수를 적용해보자\n",
        "    # 이때 softmax 산출 결과를 output 변수로 저장하고 함수의 리턴하자.\n",
        "    softmax = torch.nn.Softmax()\n",
        "    output = softmax(outputs)\n",
        "    \n",
        "    return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kwiuyp0BruXy"
      },
      "outputs": [],
      "source": [
        "if __name__ == '__main__':\n",
        "  while True:\n",
        "    input_str = input()\n",
        "\n",
        "    # q를 입력하면 while 루프 끝\n",
        "    if input_str == \"q\":\n",
        "      break\n",
        "\n",
        "    # 입력받은 문장을 토큰화\n",
        "    tokenizer = KoBertTokenizer.from_pretrained(\"monologg/kobert\")\n",
        "    tokenized_data = [data_to_token_ids(tokenizer, input_str)]\n",
        "    attention_masks = [token_ids_to_mask(token_ids) for token_ids in tokenized_data]\n",
        "\n",
        "    # 토큰을 텐서로 변환\n",
        "    inputs_tensor = torch.tensor(tokenized_data).to(device)\n",
        "    masks_tensor = torch.tensor(attention_masks).to(device)\n",
        "\n",
        "    # 신경망의 Forward 함수 활용\n",
        "    # 위의 두 텐서와 'model'의 forward 함수를 이용해서 감정에 대한 확률 벡터를 산출해보자.\n",
        "    # 이때 산출된 확률 벡터를 outputs 변수로 저장하자.\n",
        "    ## 여기에 코드 작성\n",
        "\n",
        "    # 위에서 산출한 감정 확률 벡터에 softmax 함수를 적용해보자\n",
        "    # 이때 softmax 산출 결과를 output 변수로 저장하고 함수의 리턴하자.\n",
        "    ## 여기에 코드 작성\n",
        "\n",
        "    # 두 개 해서 2점\n",
        "  "
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "nlp_main_final.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}